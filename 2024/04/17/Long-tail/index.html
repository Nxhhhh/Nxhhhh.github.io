<!DOCTYPE html>
<html  lang="zh-CN" >
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <title>Delving into the Trajectory Long-tail Distribution for Muti-object Tracking 总结 | 小牛学习日记</title>
    <meta name="description" content="贴一个原文链接 代码链接  Abstract多目标跟踪一直以来都集中于跟踪算法的的发展和后处理技术的增强，没有对跟踪数据本身的性质进行探索，作者对跟踪数据的分布模式进行探索，发现现有的MOT数据集存在显著的长尾分布问题。作者根据这一发现引入了数据增强策略略————包括静止相机视图数据增强(SVA)和动态相机视图数据增强(DVA)，这两种策略分别针对视点状态和组Softmax (GS)模块设计用于R">
<meta property="og:type" content="article">
<meta property="og:title" content="Delving into the Trajectory Long-tail Distribution for Muti-object Tracking 总结">
<meta property="og:url" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/index.html">
<meta property="og:site_name" content="小牛学习日记">
<meta property="og:description" content="贴一个原文链接 代码链接  Abstract多目标跟踪一直以来都集中于跟踪算法的的发展和后处理技术的增强，没有对跟踪数据本身的性质进行探索，作者对跟踪数据的分布模式进行探索，发现现有的MOT数据集存在显著的长尾分布问题。作者根据这一发现引入了数据增强策略略————包括静止相机视图数据增强(SVA)和动态相机视图数据增强(DVA)，这两种策略分别针对视点状态和组Softmax (GS)模块设计用于R">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/1.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/2.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/3.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/4.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/5.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/6.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/7.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/8.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/9.png">
<meta property="article:published_time" content="2024-04-17T06:17:55.000Z">
<meta property="article:modified_time" content="2024-04-17T12:28:58.784Z">
<meta property="article:author" content="Xinhang Niu">
<meta property="article:tag" content="MOT">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/1.png">

    
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

    
<link rel="stylesheet" href="/css/common.min.css">



    
        <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
    
    
    
    
        <link href="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/css/lightgallery.min.css" rel="stylesheet">
    
    
    
<link rel="stylesheet" href="/css/iconfont.min.css">

    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <header class="header header-fixture">
    <div class="profile-search-wrap flex sm:block">
        
        
        <div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6">
            <a id="avatar" role="link" href="" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer" >
                <img src="/" class="rounded-full" alt="avatar">
            </a>
            <h2 id="name" class="hidden lg:block">Niuxinhang</h2>
            <h3 id="title" class="hidden lg:block">小牛的学习日记</h3>
            
        </div>
        
        

        <button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false">
            <i class="iconfont icon-hamburger"></i>
        </button>
    </div>
    <nav id="menu-nav" class="hidden sm:flex flex-col">
        
        
            <div class="menu-item menu-home" role="menuitem">
                <a href="/.">
                    <i class="iconfont icon-home" aria-hidden="true"></i>
                    <span class="menu-title">首页</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-archives" role="menuitem">
                <a href="/archives">
                    <i class="iconfont icon-archive" aria-hidden="true"></i>
                    <span class="menu-title">归档</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-categories" role="menuitem">
                <a href="/categories">
                    <i class="iconfont icon-folder" aria-hidden="true"></i>
                    <span class="menu-title">分类</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-tags" role="menuitem">
                <a href="/tags">
                    <i class="iconfont icon-tag" aria-hidden="true"></i>
                    <span class="menu-title">标签</span>
                </a>
            </div>
        
        

    </nav>
</header>

        <section class="main-section">
            
    <main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen">
    

    <article class="content article article-archives article-type-list" itemscope="">
        <header class="article-header">
            
    
        <h1 class="article-title text-lg" itemprop="name">
            Delving into the Trajectory Long-tail Distribution for Muti-object Tracking 总结
        </h1>
    



            <p class="article-meta mb-3 text-xs">
                <span class="article-date">
    <i class="iconfont icon-calendar-check"></i>
	<a href="/2024/04/17/Long-tail/" class="article-date">
	  <time datetime="2024-04-17T06:17:55.000Z" itemprop="datePublished">4月 17</time>
	</a>
</span>

                
    <span class="article-category">
    <i class="iconfont icon-folder"></i>
    <a class="article-category-link" href="/categories/R-D/">R&D</a>
  </span>


                
    <span class="article-tags">
    <i class="iconfont icon-tag"></i>
    <a class="article-tag-none-link" href="/tags/MOT/" rel="tag">MOT</a>, <a class="article-tag-none-link" href="/tags/python/" rel="tag">python</a>
  </span>


                <span class="_partial/post-comment"><i class="icon icon-comment"></i>
                    <a href="/2024/04/17/Long-tail/#comments" class="article-comment-link">
                        评论
                    </a>
                </span>
                
    
        <span class="post-wordcount" itemprop="wordCount">字数统计: 3k(字)</span>
    
    
        <span class="post-readcount" itemprop="timeRequired">阅读时长: 11(分)</span>
    


            </p>
        </header>
        <div class="marked-body article-body">
            <p>贴一个<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.04700">原文链接</a></p>
<p>代码<a href="https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT">链接</a></p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>多目标跟踪一直以来都集中于跟踪算法的的发展和后处理技术的增强，没有对跟踪数据本身的性质进行探索，作者对跟踪数据的分布模式进行探索，发现现有的MOT数据集存在显著的长尾分布问题。作者根据这一发现引入了数据增强策略略————包括静止相机视图数据增强(SVA)和动态相机视图数据增强(DVA)，这两种策略分别针对视点状态和组Softmax (GS)模块设计用于Re-ID。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>作者进行了一项分析实验，对MOTChallenge数据集中不同身份的行人帧数进行统计。如图1所示，作者观察到不同行人身份的帧数差异很大。由于长尾分布的特征是头类具有大量实例而尾类只有少量实例，因此作者得出行人身份的数量服从长尾分布的结论。<br><img src="1.png" alt="图一"></p>
<p>长尾分布数据有一个共同的问题：在长尾分布数据上训练的网络通常会导致偏向于学习与流行的头类相关的特征，而忽略了那些较少代表性的尾类。</p>
<p>对于目前多目标跟踪器的Re-ID分支，大多将Re-ID作为分类问题，使用softmax模块计算分类概率。但是softmax模块有一个巨大的缺陷:权值大的类的权值变大，权值小的类的权值变小，这将加剧长尾分布数据上的长尾分布效应。</p>
<p>针对这一问题，作者从信息增强和模块改进两个方向提出解决方案。</p>
<p>对于信息增强，作者根据相机运动状态将摄像机数据分为静态摄像机视图和动态摄像机视图。对于静止摄像机视图，作者提出了静态摄像机视图数据增强(SVA)策略，该策略包括回溯延拓和预测延拓两种技术。对训练序列数据中间帧的尾类行人进行回溯延拓，对训练序列数据最后帧的尾类行人进行预测延拓。该策略可以促进网络对尾类行人轨迹的学习。针对动态摄像机视图数据，提出了动态摄像机视图数据增强(DVA)策略。该策略利用扩散模型对场景背景进行风格转换，提高了网络对行人区域特征的关注。</p>
<p>在模块增强方面，作者设计了Group Softmax (GS)模块。GS将训练样本数量相近的行人分组在一起，然后分别计算每组的softmax和交叉熵损失，避免了头部类权重对尾部类的显著抑制，提高了网络提取尾部类外观特征的能力。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>作者选择JDE范式，整体流程图如下所示：<br><img src="2.png" alt="流程图"></p>
<h3 id="Camera-View-Data-Augmentation"><a href="#Camera-View-Data-Augmentation" class="headerlink" title="Camera View Data Augmentation"></a>Camera View Data Augmentation</h3><p>作者将相机数据分为静态相机数据和动态相机数据，分别采用了不同的数据增强方法。</p>
<p>定义了如下所示的类别划分公式，采用该公式将行人类别划分为头部和尾部。</p>
<script type="math/tex; mode=display">
C_i = 
\begin{cases}
C_i^{tail},  & {\frac{1}{R_i} \ge T_j} \\\\
C_i^{head}, & {\frac{1}{R_i} \lt T_j}
\end{cases}</script><p>其中 $C_i$ 表示类别i所属的类别， $R_i$ 表示类别 $i$ 的数量与 $j$ 序列中所有类别的数量之比， $T_j$ 表示 $j$ 序列中判断类别是否为尾的类别阈值。</p>
<h4 id="Stationary-Camera-View-Data-Augmentation"><a href="#Stationary-Camera-View-Data-Augmentation" class="headerlink" title="Stationary Camera View Data Augmentation"></a>Stationary Camera View Data Augmentation</h4><p>专门针对尾部类别的行人进行数量增强。<br>SVA包括回溯延拓和预测延拓，如图三所示。回溯延续是在原轨迹结束后的后续帧中加入反向的原轨迹，应用于训练序列数据中间帧的尾部类行人。预测延拓是在原轨迹开始的前一帧中加入利用原轨迹位置信息预测的未来轨迹，用于训练序列数据的最后一帧中尾类行人。<br><img src="3.png" alt="图三"></p>
<h5 id="Backtracking-continuation（回溯延拓）"><a href="#Backtracking-continuation（回溯延拓）" class="headerlink" title="Backtracking continuation（回溯延拓）"></a>Backtracking continuation（回溯延拓）</h5><p>对于一个共有 $F_{total}$ 帧的训练视频，如果尾部类别的行人轨迹在第 $m$ 帧出现，在第 $n$ 帧消失，且满足 $n \lt F_{total}$ 的条件，则使用Segment Anything Model (SAM)算法对第 $m$ 到 $n$ 帧出现的行人图像区域进行分割，然后将这些图像区域以相反的顺序叠加到第 $n$ 帧之后的帧上。回溯延拓可表示为：</p>
<script type="math/tex; mode=display">
BP_j^k = P_i^k(m \le i \le n, n+1 \le j \le BF_{end})</script><p>其中，$BP_k^j$ 表示训练数据中第 $k$ 个行人在第 $j$ 帧的回溯图像位置， $P_i^k$ 表示训练数据中第 $k$ 个行人在第 $i$ 帧的图像位置， $BF_{end}$ 为训练数据中回溯延拓的截止帧，其值为 $F_{total}$ 和 $(2n−m)$ 的最小值。</p>
<h5 id="Prediction-continuation（预测延拓）"><a href="#Prediction-continuation（预测延拓）" class="headerlink" title="Prediction continuation（预测延拓）"></a>Prediction continuation（预测延拓）</h5><p>对于总共有 $F_{total}$ 帧的训练视频，如果尾类的行人轨迹出现在最后一帧，我们将从第 $m$ 帧到第 $F_{total}$ 帧出现的行人的 $x$ 和 $y$ 图像坐标输入到卡尔曼滤波器中，以预测行人后续的 $x$ 和 $y$ 图像坐标，同时确保预测的图像坐标落在图像大小范围内。在该行人轨迹中，我们随机选择能见度不小于能见度阈值 $T_v$ 的行人，其中 $0 \le T_v \le 1$ ，使用SAM算法对行人进行分割。根据从预测图像坐标中随机选择的预测 $x$ 和 $y$ 图像坐标，将分割后的图像区域叠加在行人轨迹出现前的帧上。预测延拓可表示为:</p>
<script type="math/tex; mode=display">
KP_j^k = R(KF \big (P_i^k\big)) (m \le i \le F_{total}, KF_{start} \le j \le m)</script><p>其中，$KP_j^k$ 表示卡尔曼滤波预测训练数据中第 $k$ 个行人在第 $j$ 帧的图像位置， $R()$ 表示随机选择图像位置的函数，$KF \big (P_i^k\big)$ 表示卡尔曼滤波利用训练数据中第 $i$ 个行人从第 $m$ 帧到 $F_{total}$ 的图像坐标预测的图像位置， $P_i^k$ 表示训练数据中第 $k$ 个行人在第 $i$ 帧的图像位置。 $KF_{start}$ 为训练数据中预测延拓的起始帧，其值为1和 $(2n−m)$ 的最大值。</p>
<h4 id="Dynamic-Camera-View-Data-Augmentation"><a href="#Dynamic-Camera-View-Data-Augmentation" class="headerlink" title="Dynamic Camera View Data Augmentation"></a>Dynamic Camera View Data Augmentation</h4><p>动态相机捕获的数据具有场景和主体尺寸变化明显的特点，为了解决这个问题，我们提出了动态摄像机视图数据增强(DVA)策略，如图4所示。该策略包括四个主要步骤:图像分割、图像修复、图像扩散和图像合并。首先使用图像分割算法SAM对序列导出的输入图像中的行人进行分离，得到不包含行人的图像、带有行人遮挡的图像和仅包含行人区域的图像。接下来，使用图像补绘算法Navier-Stokes对去除了行人的图像进行修复，得到修复后的图像。然后，使用稳定扩散对修复后的图像进行处理，得到扩散图像。最后，将前一步分割得到的带有行人遮挡的图像和仅包含行人区域的图像与扩散后的图像合并，生成输出图像。<br><img src="4.png" alt="图四"></p>
<h5 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h5><p>SAM是Segment Anything Model的缩写，是Meta迄今为止发布的最大的细分模型。该模型通过将提示和图像作为输入来分割对象。考虑到多目标跟踪数据集提供了边界框注释，但缺乏行人的掩码标签，我们利用图像及其相应的行人边界框标签作为SAM的输入来分割图像中的行人。</p>
<h5 id="图像修复"><a href="#图像修复" class="headerlink" title="图像修复"></a>图像修复</h5><p>在本文中，用于图像绘制的算法是基于Navier-Stokes方程。</p>
<p>该算法从待修补区域的边缘开始对图像进行修复，沿轮廓线传播图像的平滑度，待所有信息传播完毕后得到修复后的图像。</p>
<h5 id="图像扩散"><a href="#图像扩散" class="headerlink" title="图像扩散"></a>图像扩散</h5><p>稳定扩散是潜扩散模型(Latent Diffusion Model, LDM)的一种，潜扩散模型是一类能够生成新图像的去噪扩散概率模型。</p>
<p>原则上，稳定扩散可以模拟条件分布。这可以通过输入文本、语义映射或其他图像到图像转换任务信息来控制条件去噪自动编码器来实现。在本文中，作者利用输入图像通过调整提示系数和增强系数来生成新图像。</p>
<h5 id="图像合并"><a href="#图像合并" class="headerlink" title="图像合并"></a>图像合并</h5><p>我们对带有行人掩模的图像与扩散图像进行位与运算，有效地将扩散图像中与原始行人区域对应的像素值设置为0，而保持行人区域外区域的像素值不变。这就产生了后处理的扩散图像。然后，我们在后处理的扩散图像和仅包含行人的图像之间执行位或操作。实际上，该操作涉及将扩散图像中与原始行人区域对应的像素值设置为仅包含行人的图像的相应像素值，从而得到输出图像。</p>
<p>在使用扩散模型生成的增强数据训练模型时，常常存在过度强调伪特征的风险。为了解决这一问题，常见的解决方案是为原始数据和增强数据分配不同的采样概率，以此来管理数据的不平衡性。我们采用了类似的方法来平衡原始图像和由DVA（动态相机视角数据增强）生成的增强图像。从数学角度，这一方法可以表述为：</p>
<script type="math/tex; mode=display">
I_i^n = 
\begin{cases}
I_i,  & {P_i^n \le T_s} \\\\
\tilde{I_i}, & {P_i^n \gt T_s}
\end{cases}</script><p>其中 $I_i^n$ 表示第 $n$ 个训练周期中索引为 $i$ 的图像,  $I_i$ 表示索引为 $i$ 的原始图像， $ \tilde{I_i}$ 表示索引为 $i$ 的增强图像。 $P_i^n$ 表示在第 $n$ 个训练周期中索引为 $i$ 的图像被采选为原始图像的概率。 $T_s$ 表示每次图像选择时调用原始图像的阈值。给定索引 $i$ ，将原始图像  $I_i$ 以概率 $T_s$ 加到第 $n$ 个训练周期中，否则将其增广图像 $ \tilde{I_i}$ 加到第 $n$ 个训练周期中。</p>
<h3 id="Group-Softmax-Module"><a href="#Group-Softmax-Module" class="headerlink" title="Group Softmax Module"></a>Group Softmax Module</h3><p>Re-ID对不同数量的行人类别有不同程度的特征学习，对于数量较多的类别(头部类)，它往往表现得更好，而对于数量较少的类别(尾部类)，它的效率较低，这可能会对Re-ID的性能产生负面影响。为了解决这个问题，作者提出了Group Softmax (GS)模块。</p>
<p>GS将行人类别划分为若干不相交的组，并对每组分别进行softmax操作。这样，数量相近的行人类别就可以在同一组中竞争。因此，GS可以隔离数量差异显著的类别，防止尾部类别的权重受到头部类别的严重抑制。</p>
<p>具体来说，作者将训练数据集中 $M$ 类行人根据其在训练数据集中的数量分成 $K$ 个不同的组，划分规则为： $T_j^l \le N(i) \le T_j^h i \in (1,M), j \in (1,K)$ ，$N(i)$ 为训练数据集中第 $i$ 类行人的数量， $T_j^l$ 为第 $j$ 组的最低数量阈值， $T_j^h$ 为第 $j$ 组的最高数量阈值。<br>为了保证每类行人只分配给一个组，指定 $T_{j+1}^l = T_j^h+1$。</p>
<script type="math/tex; mode=display">
T_j^h = \frac{j}{K}max(N(i))(1 \le i \le M, 1 \le j \le K)</script><p>此外，对每个组分别做softmax处理，并利用交叉熵损失计算组损失，并将组损失的均值记为Re-ID损失：</p>
<script type="math/tex; mode=display">
Loss_{Re-ID} = -\frac{1}{K} \sum_{j=1}^K \sum_{i \in G_j }y_ilog(p_i)</script><p>其中 $G_j$ 表示第 $j$ 组，$y_j$ 表示 $G_j$ 中的标签， $p_j$ 表示 $G_j$ 中的概率。</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="5.png" alt><br><img src="6.png" alt><br><img src="7.png" alt><br><img src="8.png" alt><br><img src="9.png" alt></p>
<hr>

        </div>
        
<blockquote class="copyright">
    <p><strong>本文链接 : </strong><a class="permalink" href="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/">https://github.com/Nxhhhh/Nxhhhh.github.io/2024/04/17/Long-tail/</a></p>
    <p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p>
</blockquote>


    </article>
    
    <section id="comments">
        
    </section>


    

</main>


<aside style="" id="sidebar" class="aside aside-fixture">
    <div class="toc-sidebar">
        <nav id="toc" class="article-toc">
            <h3 class="toc-title">文章目录</h3>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-number">3.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Camera-View-Data-Augmentation"><span class="toc-number">3.1.</span> <span class="toc-text">Camera View Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Stationary-Camera-View-Data-Augmentation"><span class="toc-number">3.1.1.</span> <span class="toc-text">Stationary Camera View Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Backtracking-continuation%EF%BC%88%E5%9B%9E%E6%BA%AF%E5%BB%B6%E6%8B%93%EF%BC%89"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">Backtracking continuation（回溯延拓）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Prediction-continuation%EF%BC%88%E9%A2%84%E6%B5%8B%E5%BB%B6%E6%8B%93%EF%BC%89"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">Prediction continuation（预测延拓）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dynamic-Camera-View-Data-Augmentation"><span class="toc-number">3.1.2.</span> <span class="toc-text">Dynamic Camera View Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">图像分割</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">图像修复</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E6%89%A9%E6%95%A3"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">图像扩散</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%90%88%E5%B9%B6"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">图像合并</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-Softmax-Module"><span class="toc-number">3.2.</span> <span class="toc-text">Group Softmax Module</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiment"><span class="toc-number">4.</span> <span class="toc-text">Experiment</span></a></li></ol>
        </nav>
    </div>
</aside>





        </section>
        <footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40">
    
    
</footer>

        <div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div>
        <div id="search-view-container" class="hidden shadow-xl"></div>
        
<script src="/js/dom-event.min.js"></script>



<script src="/js/local-search.min.js"></script>



    <script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script>
    
<script src="/js/light-gallery.min.js"></script>






    </body>
</html>
