<!DOCTYPE html>
<html  lang="zh-CN" >
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <title>代价函数，损失函数，目标函数详解 | 小牛学习日记</title>
    <meta name="description" content="总结一下机器学习中的代价函数、损失函数、目标函数的概念以及一些典型的函数。 概念代价函数：代价函数是用来衡量模型预测输出与实际标签之间差异的一个数学表达式。在机器学习中，它的目的是最小化模型的预测误差，以找到最佳的模型参数。 损失函数：损失函数本质上与代价函数是同一个概念，也是用来度量模型预测错误的程度。可以用来表述单个样本的损失，也可以表征整个训练集的损失的平均。 目标函数：目标函数是优化问题中">
<meta property="og:type" content="article">
<meta property="og:title" content="代价函数，损失函数，目标函数详解">
<meta property="og:url" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="小牛学习日记">
<meta property="og:description" content="总结一下机器学习中的代价函数、损失函数、目标函数的概念以及一些典型的函数。 概念代价函数：代价函数是用来衡量模型预测输出与实际标签之间差异的一个数学表达式。在机器学习中，它的目的是最小化模型的预测误差，以找到最佳的模型参数。 损失函数：损失函数本质上与代价函数是同一个概念，也是用来度量模型预测错误的程度。可以用来表述单个样本的损失，也可以表征整个训练集的损失的平均。 目标函数：目标函数是优化问题中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/1.jpeg">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/2.jpeg">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/3.jpeg">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/4.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/5.png">
<meta property="og:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/6.png">
<meta property="article:published_time" content="2024-06-17T08:08:12.000Z">
<meta property="article:modified_time" content="2024-06-18T05:48:56.678Z">
<meta property="article:author" content="Xinhang Niu">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/1.jpeg">

    
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

    
<link rel="stylesheet" href="/css/common.min.css">



    
        <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
    
    
    
    
        <link href="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/css/lightgallery.min.css" rel="stylesheet">
    
    
    
<link rel="stylesheet" href="/css/iconfont.min.css">

    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <header class="header header-fixture">
    <div class="profile-search-wrap flex sm:block">
        
        
        <div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6">
            <a id="avatar" role="link" href="" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer" >
                <img src="https://www.gravatar.com/avatar/2f602fee60513793a9d9ce0b4f3e0d3b?s=128" class="rounded-full" alt="avatar">
            </a>
            <h2 id="name" class="hidden lg:block">Niuxinhang</h2>
            <h3 id="title" class="hidden lg:block">小牛的学习日记</h3>
            
        </div>
        
        
<div class="search flex-1 flex lg:inline-block sm:hidden lg:px-4 lg:mt-2 lg:mb-4 lg:w-full">
    <form id="search-form" class="my-auto flex-1 lg:border lg:border-solid lg:border-gray-200">
        <div class="input-group table bg-gray-100 lg:bg-white w-full">
            <input id="search-input" type="text" placeholder="搜索" class="inline-block w-full bg-gray-100 lg:bg-white p-1">
            <span class="table-cell">
                <button name="search tigger button" disabled>
                    <i class="iconfont icon-search m-2"></i>
                </button>
            </span>
        </div>
    </form>
        
<div id="content-json" data-placeholder="搜索" class="invisible hidden">/content.json</div>
<script id="search-teamplate" type="text/html" data-path="/content.json">
    <div>
        <div class="search-header bg-gray-400">
            <input id="actual-search-input" model="keyword" ref="input" class="inline-block w-full h-10 px-2 py-1" placeholder="搜索" type="text">
        </div>
        <div class="search-result bg-gray-200">
            {{#each searchPosts}}
            <a href="/{{ path }}" class="result-item block px-2 pb-3 mb-1 pt-1 hover:bg-indigo-100">
                <i class="iconfont icon-file"></i>
                <h1 class="result-title inline font-medium text-lg">{{ title }}</h1>
                <p class="result-content text-gray-600 text-sm">{{{ text }}}</p>
            </a>
            {{/each}}
        </div>
    </div>
</script>

</div>


        <button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false">
            <i class="iconfont icon-hamburger"></i>
        </button>
    </div>
    <nav id="menu-nav" class="hidden sm:flex flex-col">
        
        
            <div class="menu-item menu-home" role="menuitem">
                <a href="/.">
                    <i class="iconfont icon-home" aria-hidden="true"></i>
                    <span class="menu-title">首页</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-archives" role="menuitem">
                <a href="/archives">
                    <i class="iconfont icon-archive" aria-hidden="true"></i>
                    <span class="menu-title">归档</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-categories" role="menuitem">
                <a href="/categories">
                    <i class="iconfont icon-folder" aria-hidden="true"></i>
                    <span class="menu-title">分类</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-tags" role="menuitem">
                <a href="/tags">
                    <i class="iconfont icon-tag" aria-hidden="true"></i>
                    <span class="menu-title">标签</span>
                </a>
            </div>
        
        

    </nav>
</header>

        <section class="main-section">
            
    <main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen">
    

    <article class="content article article-archives article-type-list" itemscope="">
        <header class="article-header">
            
    
        <h1 class="article-title text-lg" itemprop="name">
            代价函数，损失函数，目标函数详解
        </h1>
    



            <p class="article-meta mb-3 text-xs">
                <span class="article-date">
    <i class="iconfont icon-calendar-check"></i>
	<a href="/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/" class="article-date">
	  <time datetime="2024-06-17T08:08:12.000Z" itemprop="datePublished">6月 17</time>
	</a>
</span>

                
    <span class="article-category">
    <i class="iconfont icon-folder"></i>
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </span>


                
    <span class="article-tags">
    <i class="iconfont icon-tag"></i>
    <a class="article-tag-none-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
  </span>


                <span class="_partial/post-comment"><i class="icon icon-comment"></i>
                    <a href="/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/#comments" class="article-comment-link">
                        评论
                    </a>
                </span>
                
    
        <span class="post-wordcount" itemprop="wordCount">字数统计: 1.5k(字)</span>
    
    
        <span class="post-readcount" itemprop="timeRequired">阅读时长: 5(分)</span>
    


            </p>
        </header>
        <div class="marked-body article-body">
            <p>总结一下机器学习中的代价函数、损失函数、目标函数的概念以及一些典型的函数。</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>代价函数：代价函数是用来衡量模型预测输出与实际标签之间差异的一个数学表达式。在机器学习中，它的目的是最小化模型的预测误差，以找到最佳的模型参数。</p>
<p>损失函数：损失函数本质上与代价函数是同一个概念，也是用来度量模型预测错误的程度。可以用来表述单个样本的损失，也可以表征整个训练集的损失的平均。</p>
<p>目标函数：目标函数是优化问题中想要最小化或最大化的函数，它定义了模型学习的目标。尤其是在监督学习任务中，我们的目标就是最小化模型的损失，即损失函数或代价函数。不过，目标函数的定义可以更宽泛，它也可以包含正则化项（如L1、L2正则化），用以防止模型过拟合，此时的目标函数就不仅仅是简单的损失函数了。</p>
<h1 id="典型函数"><a href="#典型函数" class="headerlink" title="典型函数"></a>典型函数</h1><h2 id="均方差损失（Mean-Squared-Error-Loss）"><a href="#均方差损失（Mean-Squared-Error-Loss）" class="headerlink" title="均方差损失（Mean Squared Error Loss）"></a>均方差损失（Mean Squared Error Loss）</h2><p>均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下：</p>
<script type="math/tex; mode=display">
J_{MSE}=\frac{1}{N} \sum^N_{i=1}(y_i-\hat{y}_i)^2</script><p>从直觉上理解均方差损失，这个损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值  ，不同的预测值  的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差  的增加，均方差损失呈二次方地增加。<br><img src="1.jpeg" alt><br>在模型输出与真实值的误差服从高斯分布的假设下，在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
<h2 id="平均绝对误差损失（Mean-Absolute-Error-Loss）"><a href="#平均绝对误差损失（Mean-Absolute-Error-Loss）" class="headerlink" title="平均绝对误差损失（Mean Absolute Error Loss）"></a>平均绝对误差损失（Mean Absolute Error Loss）</h2><p>平均绝对误差 Mean Absolute Error (MAE) 是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下：</p>
<script type="math/tex; mode=display">
J_{MAE}=\frac{1}{N} \sum^N_{i=1}|y_i-\hat{y}_i|^2</script><p>对这个损失函数进行可视化如下图，MAE 损失的最小值为 0（当预测等于真实值时），最大值为无穷大。可以看到随着预测与真实值绝对误差  的增加，MAE 损失呈线性增长：<br><img src="2.jpeg" alt></p>
<p>MSE损失收敛快但容易受outlier影响，MAE对outlier更加健壮但是收敛慢。</p>
<h2 id="交叉熵损失（Cross-Entropy-Loss）"><a href="#交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="交叉熵损失（Cross-Entropy Loss）"></a>交叉熵损失（Cross-Entropy Loss）</h2><p>交叉熵损失常用于分类问题，特别是多分类问题中。它衡量的是两个概率分布之间的差异，一个是由模型预测得到的概率分布，另一个是真实标签的概率分布（通常是独热编码）。交叉熵损失的公式如下：</p>
<script type="math/tex; mode=display">
L_{CE}=-\sum_{i=1}^Cy_ilog(p_i)</script><p>其中， $𝑦_𝑖$ 表示第𝑖类的真是标签（0或1，如果是独热编码）， $𝑝_𝑖$ 是模型预测第𝑖类的概率。</p>
<p>这个公式表达了真实标签下的对数似然的负值，目标是最大化似然，即最小化损失。</p>
<h2 id="二元交叉熵损失（Binary-Cross-Entropy-Loss）"><a href="#二元交叉熵损失（Binary-Cross-Entropy-Loss）" class="headerlink" title="二元交叉熵损失（Binary Cross-Entropy Loss）"></a>二元交叉熵损失（Binary Cross-Entropy Loss）</h2><p>在二分类问题中，二元交叉熵损失被广泛应用，公式为：</p>
<script type="math/tex; mode=display">
L_{BCE}=-\sum_{i=1}^N[y_ilog(p_i)+(1-y_i)log(1-p_i)]</script><p>下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为 0 时输出不同输出的损失，黄线是目标值为 1 时的损失。可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长。<br><img src="3.jpeg" alt></p>
<h2 id="多元交叉熵损失（Multiclass-Cross-Entropy-Loss）"><a href="#多元交叉熵损失（Multiclass-Cross-Entropy-Loss）" class="headerlink" title="多元交叉熵损失（Multiclass Cross-Entropy Loss）"></a>多元交叉熵损失（Multiclass Cross-Entropy Loss）</h2><p>多元交叉熵损失（Multiclass Cross-Entropy Loss）是用于多分类问题的一种损失函数，特别适用于神经网络和其他机器学习模型的训练过程。公式如下：</p>
<script type="math/tex; mode=display">
L_{MCE}(\hat{y},y)=-\sum y_{j}log(\hat{y}_j)</script><p>其中$\hat{y}$为预测值，${y}$为监督值。</p>
<h2 id="三元组损失（Triplet-Loss）"><a href="#三元组损失（Triplet-Loss）" class="headerlink" title="三元组损失（Triplet Loss）"></a>三元组损失（Triplet Loss）</h2><p>主要是用于训练差异性小的样本，比如人脸等。公式如下：</p>
<script type="math/tex; mode=display">
L_{T}=max(d_p-d_n+ \alpha, 0)</script><p>其中，$d_p$ $d_n$ 分别是正对（同一身份的两个实例）和负对（不同身份的实例对）之间的距离，𝛼是一个超参数，即边际。</p>
<p>其中样本可以分为三类：</p>
<p>easy triplets： $L=0$，即 $d_p+ \alpha &lt;d_n$ 这种情况不需要优化，如下图:<br><img src="4.png" alt></p>
<p>hard triplets： $L&gt;𝛼$，即 $d_p &lt; d_n$ ，这种情况损失最大，需要优化，如下图:<br><img src="5.png" alt></p>
<p>semi-hard triplets： $L&lt;𝛼$，即 $d_p &lt; d_n &lt; d_p+ \alpha $ ，这种情况存在损失，但损失比hard triplets要小，也需要优化，如下图：<br><img src="6.png" alt></p>
<h2 id="中心损失（Center-Loss）"><a href="#中心损失（Center-Loss）" class="headerlink" title="中心损失（Center Loss）"></a>中心损失（Center Loss）</h2><p>中心损失的基本思想是为每个类别学习一个中心（或称为类别原型），然后在训练过程中，除了传统的分类损失（如交叉熵损失）外，还加入一个额外的惩罚项，以减小每个样本特征向量与对应类别中心之间的距离。这样做可以提升模型的识别能力，特别是在面对类别间相似度高或类别内差异大的情况时。</p>
<p>中心损失它仅仅用来减少类内（比如说同一表情）的差异，而不能有效增大类间（比如说不同表情）的差异性，公式如下：</p>
<script type="math/tex; mode=display">
L_{C}=\frac{1}{N}\sum_{i=1}^N ||f_i-C_{y_i}||^2</script><p>其中$N$是样本总数，$f_i$ 表示特征提取网络输出的特征向量， $C_{y_i}$ 表示对应的类别中心， $||·||$ 表示欧氏距离。</p>

        </div>
        
<blockquote class="copyright">
    <p><strong>本文链接 : </strong><a class="permalink" href="https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/">https://github.com/Nxhhhh/Nxhhhh.github.io/2024/06/17/代价函数，损失函数，目标函数详解/</a></p>
    <p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p>
</blockquote>


    </article>
    
    <section id="comments">
        
    </section>


    

</main>


<aside style="" id="sidebar" class="aside aside-fixture">
    <div class="toc-sidebar">
        <nav id="toc" class="article-toc">
            <h3 class="toc-title">文章目录</h3>
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B8%E5%9E%8B%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">典型函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E5%B7%AE%E6%8D%9F%E5%A4%B1%EF%BC%88Mean-Squared-Error-Loss%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">均方差损失（Mean Squared Error Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1%EF%BC%88Mean-Absolute-Error-Loss%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">平均绝对误差损失（Mean Absolute Error Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Cross-Entropy-Loss%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">交叉熵损失（Cross-Entropy Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%85%83%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Binary-Cross-Entropy-Loss%EF%BC%89"><span class="toc-number">2.4.</span> <span class="toc-text">二元交叉熵损失（Binary Cross-Entropy Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Multiclass-Cross-Entropy-Loss%EF%BC%89"><span class="toc-number">2.5.</span> <span class="toc-text">多元交叉熵损失（Multiclass Cross-Entropy Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1%EF%BC%88Triplet-Loss%EF%BC%89"><span class="toc-number">2.6.</span> <span class="toc-text">三元组损失（Triplet Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E5%BF%83%E6%8D%9F%E5%A4%B1%EF%BC%88Center-Loss%EF%BC%89"><span class="toc-number">2.7.</span> <span class="toc-text">中心损失（Center Loss）</span></a></li></ol></li></ol>
        </nav>
    </div>
</aside>





        </section>
        <footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40">
    
    
</footer>

        <div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div>
        <div id="search-view-container" class="hidden shadow-xl"></div>
        
<script src="/js/dom-event.min.js"></script>



<script src="/js/local-search.min.js"></script>



    <script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script>
    
<script src="/js/light-gallery.min.js"></script>






    </body>
</html>
